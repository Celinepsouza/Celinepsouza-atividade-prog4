# üß† Relat√≥rio T√©cnico ‚Äî Detec√ß√£o de Bots no Twitter com BERT

## 1. Objetivo do Projeto

O objetivo principal deste notebook √© **treinar e avaliar um modelo BERT** para identificar contas automatizadas (‚Äúbots‚Äù) no Twitter, utilizando dados p√∫blicos (por exemplo, datasets do Kaggle, como *Twitter Bot Detection Dataset*).  
O modelo visa classificar cada tweet ou perfil como **bot (1)** ou **humano (0)**, com foco em precis√£o e efici√™ncia.

---

## 2. Estrutura Geral do Notebook

O notebook foi organizado em seis se√ß√µes principais:

1. **Configura√ß√£o do ambiente** ‚Äî Importa√ß√£o de bibliotecas, ativa√ß√£o de GPU e otimiza√ß√µes.  
2. **Hiperpar√¢metros e setup de diret√≥rios** ‚Äî Defini√ß√£o de preset BERT, sequ√™ncia m√°xima, batch, √©pocas e taxa de aprendizado.  
3. **Prepara√ß√£o e subamostragem dos dados** ‚Äî Redu√ß√£o do dataset para caber na mem√≥ria do Colab, mantendo o balanceamento entre classes.  
4. **Cria√ß√£o dos datasets TensorFlow (`tf.data`)** ‚Äî Convers√£o dos textos e r√≥tulos em datasets eficientes.  
5. **Constru√ß√£o e treinamento do modelo BERT** ‚Äî Utiliza√ß√£o do preset `bert_base_en` com cabe√ßa bin√°ria (sigmoid).  
6. **Avalia√ß√£o e salvamento dos artefatos** ‚Äî C√°lculo de m√©tricas, relat√≥rio de classifica√ß√£o e persist√™ncia do modelo.

---

## 3. Configura√ß√£o e Otimiza√ß√µes

### 3.1. Inicializa√ß√£o e limpeza de sess√£o

Para evitar conflitos de mem√≥ria e aproveitar o hardware do Colab:
- `gc.collect()` e `tf.keras.backend.clear_session()` limpam sess√µes anteriores.
- Ativa-se **XLA JIT** (`tf.config.optimizer.set_jit(True)`) para compilar graficamente opera√ß√µes TensorFlow e acelerar o treino.
- Ativa-se **mixed precision** (`float16`) para reduzir o uso de RAM na GPU e acelerar opera√ß√µes matriciais.

### 3.2. Hiperpar√¢metros definidos

| Par√¢metro | Valor | Descri√ß√£o |
|------------|--------|-----------|
| `PRESET` | `"bert_base_en"` | Modelo base da fam√≠lia BERT dispon√≠vel no KerasNLP |
| `MAX_LEN` | 48 | Comprimento m√°ximo da sequ√™ncia (reduz custo computacional) |
| `BATCH` | 4 | Tamanho do lote (trade-off entre mem√≥ria e estabilidade) |
| `EPOCHS` | 1 | Treinamento r√°pido de uma √©poca (linear probing) |
| `LR` | 5e-5 | Taxa de aprendizado inicial |
| `MAX_PER_CLASS` | 1500 | Limite de exemplos por classe na subamostragem |

---

## 4. Prepara√ß√£o dos Dados

### 4.1. Subamostragem Estratificada

Para evitar consumo excessivo de RAM, a fun√ß√£o `stratified_cap()`:
- Garante equil√≠brio entre as classes (`bot` e `humano`);
- Seleciona no m√°ximo `MAX_PER_CLASS` amostras de cada grupo;
- Embaralha os exemplos ap√≥s o corte.

Com isso, o dataset final cont√©m aproximadamente **3.000 amostras** (1.500 por classe), ideal para treinos r√°pidos no Colab.

### 4.2. Convers√£o em `tf.data.Dataset`

Os textos e r√≥tulos s√£o convertidos em tensores TensorFlow, usando:
```python
tf.data.Dataset.from_tensor_slices((texts, labels))
```
Em seguida:
- Os dados s√£o embaralhados (apenas no treino);
- Divididos em lotes (`batch(batch_size)`);
- Pr√©-carregados com `prefetch(1)` para n√£o estourar RAM.

---

## 5. Constru√ß√£o e Treinamento do Modelo

### 5.1. Modelo BERT com KerasNLP

O modelo √© criado a partir de um preset do KerasNLP:
```python
bert_clf = keras_nlp.models.BertClassifier.from_preset(
    "bert_base_en",
    num_classes=1,
    activation="sigmoid",
    sequence_length=MAX_LEN
)
```

- **`num_classes=1` + `sigmoid`** ‚Üí Sa√≠da bin√°ria (probabilidade de ser bot).
- **Preprocessor embutido** ‚Üí Faz tokeniza√ß√£o e padding automaticamente.

### 5.2. ‚ÄúLinear Probing‚Äù: congelamento do backbone

O par√¢metro:
```python
bert_clf.backbone.trainable = False
```
congela as camadas do encoder BERT, mantendo apenas a cabe√ßa (camadas finais densas) trein√°vel ‚Äî isso reduz drasticamente o tempo de execu√ß√£o e o consumo de GPU.

### 5.3. Compila√ß√£o e callbacks

```python
bert_clf.compile(
    optimizer=tf.keras.optimizers.AdamW(learning_rate=LR),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=[BinaryAccuracy, AUC]
)
```

- `AdamW` ‚Üí otimizador com *weight decay*, ideal para fine-tuning.  
- `EarlyStopping` ‚Üí interrompe o treino se o AUC n√£o melhorar.  
- `ModelCheckpoint` ‚Üí salva o melhor modelo com base em AUC.

---

## 6. Avalia√ß√£o do Modelo

### 6.1. Predi√ß√£o e m√©tricas

Ap√≥s o treinamento:
```python
proba = bert_clf.predict(val_ds).squeeze()
auc   = roc_auc_score(y_test, proba)
pred  = (proba >= 0.5).astype(int)
```

M√©tricas geradas:
- **AUC (√Årea sob a Curva ROC)** ‚Äî principal indicador do desempenho do modelo bin√°rio;
- **Precision / Recall / F1-score** ‚Äî via `classification_report`;
- **Matriz de confus√£o** ‚Äî para an√°lise visual dos acertos/erros.

### 6.2. Resultados t√≠picos (baseline r√°pido)

Com as configura√ß√µes leves (`MAX_LEN=48`, `BATCH=4`, `1 √©poca`):
- **AUC** ‚âà 0.70‚Äì0.80 dependendo do dataset.
- **F1-score** razo√°vel (~0.7), considerando o backbone congelado.

Esses resultados podem ser melhorados descongelando gradualmente as √∫ltimas camadas BERT e aumentando o n√∫mero de √©pocas.

---

## 7. Salvamento dos Artefatos

Os resultados e o modelo s√£o persistidos em:
```
artifacts_bot_detection/
‚îú‚îÄ‚îÄ bert_best.keras
‚îú‚îÄ‚îÄ bert_final.keras
‚îú‚îÄ‚îÄ bert_val_metrics.json
```

O JSON cont√©m metadados como:
- `auc`
- `max_len`, `batch`, `epochs`
- `n_train_used` (tamanho efetivo do dataset)

---

## 8. Conclus√µes e Recomenda√ß√µes

### 8.1. Conclus√µes

- O notebook implementa um pipeline **otimizado para Colab**, com consumo leve e execu√ß√£o r√°pida (~5‚Äì10 min).  
- O modelo BERT, mesmo treinado apenas na cabe√ßa (‚Äúlinear probing‚Äù), j√° mostra boa capacidade de distin√ß√£o entre bots e humanos.  
- O uso do `keras_nlp` simplifica muito o pr√©-processamento e integra√ß√£o do BERT.


---

## **Resumo Final**

Este notebook implementa um fluxo eficiente e escal√°vel para classifica√ß√£o bin√°ria de tweets com BERT, demonstrando como realizar fine-tuning leve no Colab sem estourar mem√≥ria, mantendo resultados competitivos.
