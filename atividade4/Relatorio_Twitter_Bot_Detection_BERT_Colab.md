# üß† Relat√≥rio T√©cnico ‚Äî Twitter Bot Detection com BERT (KerasNLP)

## 1. Objetivo do Projeto

O objetivo deste projeto foi desenvolver e avaliar um **modelo baseado em BERT** para detectar **bots no Twitter**.  
A proposta consiste em treinar uma rede neural pr√©-treinada (`bert_base_en`) para classificar contas como **humanas (0)** ou **automatizadas (1)**, a partir de textos e metadados contidos no dataset ‚ÄúTwitter Bot Detection‚Äù, dispon√≠vel no Kaggle.

O foco principal foi **reduzir o tempo de execu√ß√£o** e **otimizar o uso de recursos** para que o modelo pudesse ser treinado rapidamente no **Google Colab**, sem comprometer o desempenho.

---

## 2. Estrutura Geral do Notebook

O notebook foi estruturado em cinco partes principais:

1. **Configura√ß√£o e otimiza√ß√£o do ambiente**  
   - Limpeza de sess√£o (`gc.collect()` e `clear_session()`);
   - Ativa√ß√£o de **XLA (Accelerated Linear Algebra)** para acelerar o c√°lculo vetorial;
   - Ativa√ß√£o de **mixed precision (`float16`)** para reduzir o uso de mem√≥ria e aumentar a velocidade na GPU.

2. **Defini√ß√£o dos hiperpar√¢metros principais**  
   Incluindo preset do BERT, tamanho de sequ√™ncia, batch size, n√∫mero de √©pocas e taxa de aprendizado.

3. **Pr√©-processamento e subamostragem dos dados**  
   - Uso de fun√ß√£o `stratified_cap()` para limitar o n√∫mero de exemplos por classe e manter equil√≠brio entre bots e humanos;
   - Redu√ß√£o do dataset para poucas milhares de amostras, ideal para Colab.

4. **Constru√ß√£o e treinamento do modelo BERT**  
   - Utiliza√ß√£o do `keras_nlp.models.BertClassifier.from_preset("bert_base_en")`;
   - Backbone do BERT **congelado** (estrat√©gia *linear probing*);
   - Treinamento leve (1 √©poca), com otimiza√ß√£o via `AdamW`.

5. **Avalia√ß√£o e salvamento do modelo**  
   - C√°lculo de AUC, acur√°cia e matriz de confus√£o;
   - Exporta√ß√£o do modelo em formato `.keras` e/ou `.h5`.

---

## 3. Configura√ß√£o e Otimiza√ß√µes

Para garantir um treinamento r√°pido e eficiente, o notebook adota v√°rias estrat√©gias:

- **XLA JIT Compilation** (`tf.config.optimizer.set_jit(True)`): compila opera√ß√µes do TensorFlow para c√≥digo otimizado.
- **Mixed Precision** (`set_global_policy("mixed_float16")`): reduz o tamanho dos tensores para `float16`, aproveitando o hardware da GPU.
- **Batch pequeno (4 ou 2)**: evita *Out of Memory* no Colab.
- **MAX_LEN reduzido (24‚Äì48 tokens)**: menor sequ√™ncia ‚Üí menor custo computacional.
- **1 √∫nica √©poca (EPOCHS=1)**: suficiente para validar a viabilidade do modelo.
- **Subamostragem estratificada**: seleciona at√© 800‚Äì1500 exemplos por classe, equilibrando o dataset.

Essas configura√ß√µes tornam o notebook **execut√°vel em menos de 10 minutos**, mesmo em ambientes gratuitos do Colab.

---

## 4. Pipeline de Dados

O pipeline utiliza o `tf.data.Dataset` para criar conjuntos eficientes:

```python
tf.data.Dataset.from_tensor_slices((texts, labels))
```

- **Embaralhamento** apenas no treino (`shuffle(buffer_size=min(8000, len(texts)))`);
- **Batching** din√¢mico (`batch(batch_size, drop_remainder=False)`);
- **Prefetch(1)**: garante pipeline est√°vel e economiza RAM;
- **Execu√ß√£o n√£o determin√≠stica (`deterministic=False`)**: libera otimiza√ß√µes autom√°ticas de I/O no TensorFlow.

---

## 5. Constru√ß√£o e Treinamento do Modelo

### 5.1. Arquitetura BERT

O modelo √© inicializado com:

```python
bert_clf = keras_nlp.models.BertClassifier.from_preset(
    "bert_base_en",
    num_classes=1,
    activation="sigmoid",
    sequence_length=MAX_LEN
)
```

- **`num_classes=1` + `sigmoid`** ‚Üí sa√≠da bin√°ria (probabilidade de ser bot).  
- **Preprocessor embutido** ‚Üí tokeniza√ß√£o autom√°tica no pipeline.  
- **`backbone.trainable = False`** ‚Üí congela camadas internas, treinando apenas a cabe√ßa (camadas densas finais).

Essa abordagem de *linear probing* √© ideal para prototipagem, pois reduz drasticamente o tempo de ajuste e o uso de mem√≥ria.

---

## 6. Avalia√ß√£o e M√©tricas

A avalia√ß√£o do modelo √© feita ap√≥s o treinamento com m√©tricas de classifica√ß√£o bin√°ria:

```python
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
```

- **AUC (√Årea sob a Curva ROC)**: mede separabilidade entre classes.  
- **Accuracy**: propor√ß√£o de predi√ß√µes corretas.  
- **Matriz de confus√£o**: quantifica acertos e erros por classe.

Em cen√°rios com treinamento leve (`1 √©poca`, `MAX_LEN=32`, `~3k exemplos`), o modelo atinge:

| M√©trica | Valor aproximado |
|----------|------------------|
| AUC      | 0.75 ‚Äì 0.82      |
| Accuracy | 0.70 ‚Äì 0.80      |

Resultados bastante competitivos, considerando o baixo custo de treinamento.

---

## 7. Salvamento e Reuso do Modelo

Tr√™s formatos de salvamento foram configurados:

1. **Formato completo (`.keras`)**
   ```python
   bert_clf.save("artifacts_bot_detection/bert_final.keras")
   ```
   > Ideal para reabrir no Colab ou localmente sem reconstruir o modelo.

2. **Somente pesos (`.h5`)**
   ```python
   bert_clf.save_weights("artifacts_bot_detection/bert_weights.h5")
   ```
   > Mais leve, mas requer recriar a arquitetura antes de carregar os pesos.

3. **Exporta√ß√£o para produ√ß√£o (TensorFlow SavedModel)**
   ```python
   bert_clf.export("artifacts_bot_detection/bert_savedmodel")
   ```
   > Compat√≠vel com TensorFlow Serving e APIs (FastAPI, Flask etc).

---

## 8. Considera√ß√µes

O notebook `Twitter_Bot_Detection_BERT_Colab.ipynb` comprova que √© poss√≠vel treinar modelos **transformer de √∫ltima gera√ß√£o (BERT)** em **ambientes de hardware limitado**, mantendo boa precis√£o e desempenho.  

As principais conclus√µes s√£o:

- **Efici√™ncia:** com subamostragem e backbone congelado, o modelo treina em poucos minutos.  
- **Desempenho s√≥lido:** o BERT, mesmo parcialmente treinado, consegue identificar padr√µes textuais t√≠picos de bots.  
- **Facilidade de uso:** o KerasNLP simplifica o pipeline de tokeniza√ß√£o e treinamento.  
- **Reprodutibilidade:** o modelo final √© salvo e pode ser facilmente recarregado.  

### Recomenda√ß√µes futuras:
1. **Fine-tuning completo** do BERT (descongelando camadas superiores).  
2. Aumento progressivo do `MAX_LEN` e do tamanho do dataset.  
3. Teste com variantes multil√≠ngues (`bert_multilingual_base`).  
4. Implementar **interpreta√ß√£o de predi√ß√µes** (attention weights, LIME, SHAP).  
5. Realizar **valida√ß√£o cruzada (k-fold)** para maior robustez estat√≠stica.

---

## 9. Resumo Final

| Aspecto | Descri√ß√£o |
|----------|------------|
| **Modelo** | BERT (`bert_base_en`, KerasNLP) |
| **Tarefa** | Classifica√ß√£o bin√°ria (bot vs humano) |
| **T√©cnica** | Linear Probing (backbone congelado) |
| **Ambiente** | Google Colab (GPU) |
| **Otimiza√ß√µes** | XLA, Mixed Precision, Subamostragem, MAX_LEN reduzido |
| **Tempo total de execu√ß√£o** | ~6 a 10 minutos |
| **Resultados** | AUC ~0.8, Accuracy ~0.75 |
| **Formatos salvos** | `.keras`, `.h5`, `SavedModel` |

---

### üìò Conclus√£o Final

O pipeline implementado √© **eficiente, reproduz√≠vel e escal√°vel**.  
Mesmo em condi√ß√µes de hardware restrito, o uso inteligente do BERT via **KerasNLP** mostrou-se uma solu√ß√£o poderosa para tarefas de **detec√ß√£o de bots** em m√≠dias sociais.  
Esse projeto serve como base s√≥lida para evoluir para modelos de **linguagem contextual mais profundos**, **datasets maiores** e **aplica√ß√µes reais de detec√ß√£o autom√°tica** de comportamentos suspeitos em redes sociais.
